Model:
InteractionModelFC(
  (fc1): Linear(in_features=23320, out_features=1000, bias=True)
  (fc2): Linear(in_features=1000, out_features=200, bias=True)
  (fc3): Linear(in_features=400, out_features=200, bias=True)
  (fc4): Linear(in_features=200, out_features=100, bias=True)
  (fc5): Linear(in_features=100, out_features=2, bias=True)
  (norm1a): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True)
  (norm1b): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True)
  (norm3): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True)
  (norm4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True)
)
Optimizer: Adadelta
Epochs: 100
06:48:24
Size of batches: 18 
Learning rate: 0.001
Momentum: 0.1 
Training Loss over epochs
0 : 0.5509644061827448
1 : 0.49818714569970735
2 : 0.48610212292018046
3 : 0.4781304919429229
4 : 0.47220622654176286
5 : 0.4690109224334374
6 : 0.46803623101292846
7 : 0.46672530657255146
8 : 0.4651958277479064
9 : 0.46344789381791857
10 : 0.4634523159055518
11 : 0.4624736277355267
12 : 0.46214201052329396
13 : 0.46074701119968675
14 : 0.4600073819710779
15 : 0.45951289165204473
16 : 0.45893751725107973
17 : 0.4590209567962919
18 : 0.45884457737904144
19 : 0.4576997195846123
20 : 0.45837984941372323
21 : 0.45785555465267724
22 : 0.45739899087509595
23 : 0.45685794375466404
24 : 0.45623318036999166
25 : 0.4562041823766637
26 : 0.45576689290026906
27 : 0.45598444040370373
28 : 0.4553613879392521
29 : 0.45494598106094436
30 : 0.4552157597102968
31 : 0.4542544671077653
32 : 0.454925803789623
33 : 0.4537983546337094
34 : 0.4535609945718654
35 : 0.45340817552434015
36 : 0.45307639472807243
37 : 0.4531445927846812
38 : 0.4524516460433931
39 : 0.45274172385232714
40 : 0.4527692427692501
41 : 0.4520200361418889
42 : 0.45224638268042444
43 : 0.4514942359857588
44 : 0.4514739406140258
45 : 0.45114401507856505
46 : 0.4508511270097419
47 : 0.4510732429628903
48 : 0.4511213877168108
49 : 0.4508788544447908
50 : 0.45020874681188594
51 : 0.4508817782518225
52 : 0.4509312053887829
53 : 0.4499445370262612
54 : 0.4495952358401289
55 : 0.4499452071222067
56 : 0.4496307271285241
57 : 0.4495685021167607
58 : 0.4492607256535601
59 : 0.4493520799184026
60 : 0.448865593490453
61 : 0.4490843550386419
62 : 0.4495986878165376
63 : 0.4498701777491948
64 : 0.449016276358145
65 : 0.44918902688878987
66 : 0.44875649759005587
67 : 0.44853495445579616
68 : 0.44866395728698794
69 : 0.4481850741900305
70 : 0.4484446859493199
71 : 0.4482646915898497
72 : 0.4476280994602909
73 : 0.44777056788746605
74 : 0.44777759094162917
75 : 0.4477701442077594
76 : 0.44736368690700484
77 : 0.4473189915682793
78 : 0.4475751559927466
79 : 0.44748964004898134
80 : 0.44742738959190287
81 : 0.4475169469837613
82 : 0.44738788168250554
83 : 0.4472650808745558
84 : 0.4473302772082818
85 : 0.44666816719590263
86 : 0.44671672723934686
87 : 0.44705863687055536
88 : 0.44654667724854935
89 : 0.4468487171228677
90 : 0.44671954575203576
91 : 0.4462587397703812
92 : 0.44655938446031107
93 : 0.4458360987291314
94 : 0.44603248779067955
95 : 0.4461588813829155
96 : 0.44638295403467343
97 : 0.4460375593962307
98 : 0.4457571962023394
99 : 0.4456867326883882
