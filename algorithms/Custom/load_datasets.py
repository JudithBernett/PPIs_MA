import numpy as np
import random
from tqdm import tqdm
from sklearn.model_selection import train_test_split


def read_nodelist(path):
    id_dict = dict()
    with open(path, 'r') as f:
        for line in f:
            line_split = line.strip().split("\t")
            id_dict[line_split[1]] = int(line_split[0])
    return id_dict


def read_matrix_colnames(path):
    id_dict = dict()
    idx = 0
    with open(path, 'r') as f:
        for line in f:
            id_dict[line.strip()] = idx
            idx += 1
    return id_dict


def node2vec_embeddings_to_dict(embed_pth):
    """
    read the embeddings generated by Node2vec
    :return dict of id -> embeddings
    """
    # load the embeddings generated by node2vec for each index GO, ignore first information line
    embeddings_dict = {}
    embeddings = open(embed_pth).read().splitlines()[1:]
    embeddings = [x.split(" ") for x in embeddings]

    for i in range(0, len(embeddings)):
        # set the id as the key
        key = int(embeddings[i][0])
        # add all the dimension of the embedings as a list of floats
        embeddings_dict[key] = [float(x) for x in embeddings[i][1:]]

    return embeddings_dict


def load_encoding(encoding='PCA', organism='yeast'):
    dim_reds = {'PCA', 'MDS', 'node2vec'}
    if encoding not in dim_reds:
        raise ValueError("encoding must be one of %r." % dim_reds)
    if encoding == 'PCA':
        if organism == 'yeast':
            emb = np.load('data/yeast_pca.npy')
            id_dict = read_matrix_colnames('../../network_data/SIMAP2/matrices/sim_matrix_yeast_colnames.txt')
        else:
            emb = np.load('data/human_pca.npy')
            id_dict = read_matrix_colnames('../../network_data/SIMAP2/matrices/sim_matrix_human_colnames.txt')

    elif encoding == 'MDS':
        if organism == 'yeast':
            emb = np.load('data/yeast_mds.npy')
            id_dict = read_matrix_colnames('../../network_data/SIMAP2/matrices/sim_matrix_yeast_colnames.txt')
        else:
            emb = np.load('data/human_mds.npy')
            id_dict = read_matrix_colnames('../../network_data/SIMAP2/matrices/sim_matrix_human_colnames.txt')

    else:
        if organism == 'yeast':
            emb = node2vec_embeddings_to_dict('data/yeast.emb')
            id_dict = read_nodelist('data/yeast.nodelist')
        else:
            emb = node2vec_embeddings_to_dict('data/human.emb')
            id_dict = read_nodelist('data/human.nodelist')
    return emb, id_dict


def make_X_y(ppis, emb, id_dict):
    X = np.empty(shape=(len(ppis), 256))
    y = np.empty(shape=(len(ppis)), dtype=int)
    for i, ppi in enumerate(ppis):
        idx_0 = id_dict[ppi[0]]
        idx_1 = id_dict[ppi[1]]
        label = ppi[2]
        emb0 = emb[idx_0]
        emb1 = emb[idx_1]
        X[i] = np.append(emb0, emb1)
        y[i] = label
    return X, y


def load_du(encoding='PCA'):
    emd, id_dict = load_encoding(encoding=encoding, organism='yeast')
    f = open('../../Datasets_PPIs/Du_yeast_DIP/SupplementaryS1.csv').readlines()
    pos_ppis = list()
    neg_ppis = list()
    for line in tqdm(f):
        if line.startswith('proteinA'):
            # header
            continue
        line_split = line.strip().split(',')
        if line_split[2] == '1':
            if encoding == 'node2vec':
                id0 = id_dict[line_split[0]]
                id1 = id_dict[line_split[1]]
                if id0 in emd.keys() and id1 in emd.keys():
                    pos_ppis.append(line_split)
            else:
                pos_ppis.append(line_split)
        else:
            if encoding == 'node2vec':
                id0 = id_dict[line_split[0]]
                id1 = id_dict[line_split[1]]
                if id0 in emd.keys() and id1 in emd.keys():
                    neg_ppis.append(line_split)
            else:
                neg_ppis.append(line_split)
    to_delete = set(random.sample(range(len(neg_ppis)), len(neg_ppis) - len(pos_ppis)))
    neg_ppis = [x for i, x in enumerate(neg_ppis) if not i in to_delete]
    pos_ppis.extend(neg_ppis)
    ppis = np.array(pos_ppis)
    X, y = make_X_y(ppis, emd, id_dict)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    return X_train, y_train, X_test, y_test


def balance_set(ppis, id_dict, encoding, emd):
    pos_len = sum([int(pair[2]) for pair in ppis])
    neg_len = len(ppis) - pos_len
    if pos_len > neg_len:
        print(f'sampling more negatives ({pos_len} positives, {neg_len} negatives)...')
        if encoding == 'node2vec':
            candidates = [key for key, value in id_dict.items() if value in emd.keys()]
        else:
            candidates = [key for key, value in id_dict.items()]

        while pos_len > neg_len:
            prot1 = random.choice(tuple(candidates))
            prot1_list = [pair[0] for pair in ppis if pair[1] == prot1] + [pair[1] for pair in ppis if
                                                                                pair[0] == prot1]
            # protein should occur in the dataset
            while len(prot1_list) == 0:
                prot1 = random.choice(tuple(candidates))
                prot1_list = [pair[0] for pair in ppis if pair[1] == prot1] + [pair[1] for pair in ppis if
                                                                                    pair[0] == prot1]
            prot2 = random.choice(tuple(candidates))
            prot2_list = [pair[0] for pair in ppis if pair[1] == prot2] + [pair[1] for pair in ppis if
                                                                                pair[0] == prot2]
            node2vec_valid=True
            if encoding == 'node2vec' and (id_dict[prot1] not in emd.keys() or id_dict[prot2] not in emd.keys()):
                node2vec_valid=False

            while prot1 == prot2 or prot2 in prot1_list or len(prot2_list) == 0 or not node2vec_valid:
                prot2 = random.choice(tuple(candidates))
                prot2_list = [pair[0] for pair in ppis if pair[1] == prot2] + [pair[1] for pair in ppis if
                                                                                    pair[0] == prot2]
                if encoding == 'node2vec' and (id_dict[prot1] not in emd.keys() or id_dict[prot2] not in emd.keys()):
                    node2vec_valid = False
                elif encoding == 'node2vec' and id_dict[prot1] in emd.keys() and id_dict[prot2] in emd.keys():
                    node2vec_valid = True
            ppis.append([prot1, prot2, '0'])
            neg_len += 1
    else:
        print(f'randomly dropping negatives ({pos_len} positives, {neg_len} negatives)...')
        pos_ppis = [x for x in ppis if x[2] == '1']
        neg_ppis = [x for x in ppis if x[2] == '0']
        to_delete = set(random.sample(range(len(neg_ppis)), len(neg_ppis) - len(pos_ppis)))
        neg_ppis = [x for i, x in enumerate(neg_ppis) if not i in to_delete]
        ppis = pos_ppis
        ppis.extend(neg_ppis)
    return ppis


def load_guo(encoding='PCA'):
    emd, id_dict = load_encoding(encoding=encoding, organism='yeast')
    f = open('../../Datasets_PPIs/Guo_yeast_DIP/protein.actions.tsv').readlines()
    ppis = list()
    unknown_prots = []
    for line in tqdm(f):
        line_split = line.strip().split('\t')
        if line_split[0] in id_dict.keys() and line_split[1] in id_dict.keys():
            id0 = id_dict[line_split[0]]
            id1 = id_dict[line_split[1]]
            if encoding == 'node2vec' and id0 in emd.keys() and id1 in emd.keys():
                ppis.append(line_split)
            elif encoding != 'node2vec':
                ppis.append(line_split)
        else:
            if line_split[0] in id_dict.keys():
                unknown_prots.append(line_split[0])
            else:
                unknown_prots.append(line_split[1])
    print(f'{len(unknown_prots)} unknown proteins: {unknown_prots}')
    ppis = balance_set(ppis, id_dict, encoding, emd)
    X, y = make_X_y(np.array(ppis), emd, id_dict)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    return X_train, y_train, X_test, y_test


def make_swissprot_to_dict(path_to_swissprot):
    prefix_dict = {}
    seq_dict = {}
    header_line = False
    last_id = ''
    last_seq = ''
    n = 30
    f = open(path_to_swissprot, 'r')
    for line in f:
        if line.startswith('>'):
            if last_id != '':
                seq_dict[last_id] = last_seq
                last_seq = ''
            header_line = True
            uniprot_id = line.split('|')[1]
            last_id = uniprot_id
        elif header_line is True:
            last_seq += line.strip()
            first_n = line[0:n]
            if first_n in prefix_dict.keys():
                if isinstance(prefix_dict[first_n], list):
                    prefix_dict[first_n].append(last_id)
                else:
                    prefix_dict[first_n] = [prefix_dict[first_n], last_id]
            else:
                prefix_dict[first_n] = last_id
            header_line = False
        else:
            last_seq += line.strip()
    f.close()
    return prefix_dict, seq_dict


def iterate_pan(prefix_dict, seq_dict, path_to_pan):
    lines = open(path_to_pan, 'r').readlines()
    encountered_ids = []
    n = 30
    unmapped = 0
    mapping_dict = dict()
    for line in lines:
        old_id = line.strip().split('\t')[0]
        if old_id not in encountered_ids:
            encountered_ids.append(old_id)
            seq = line.strip().split('\t')[1]
            first_n = seq[0:n]
            if first_n not in prefix_dict.keys():
                uniprot_id = ''
            elif isinstance(prefix_dict[first_n], list):
                uniprot_ids = prefix_dict[first_n]
                uniprot_id = ''
                for id in uniprot_ids:
                    # just take the first mapped ID
                    if seq_dict[id] == seq:
                        uniprot_id = id
                        break
            else:
                uniprot_id = prefix_dict[first_n]
            if uniprot_id == '':
                unmapped += 1
            mapping_dict[old_id] = uniprot_id
    print(f'#unmapped IDs: {unmapped}')
    return mapping_dict


def load_pan(encoding):
    from tqdm import tqdm
    prefix_dict, seq_dict = make_swissprot_to_dict('../../network_data/Swissprot/human_swissprot.fasta')
    print('Mapping Protein IDs ...')
    mapping_dict = iterate_pan(prefix_dict, seq_dict, '../../Datasets_PPIs/Pan_human_HPRD/SEQ-Supp-ABCD.tsv')
    print(f'Loading {encoding} encoding ...')
    emd, id_dict = load_encoding(encoding=encoding, organism='human')
    ppis = []
    lines = open('../../Datasets_PPIs/Pan_human_HPRD/Supp-AB.tsv', 'r').readlines()
    for line in tqdm(lines):
        if line.startswith('v1'):
            # header
            continue
        else:
            line_split_pan = line.strip().split('\t')
            id0_pan = line_split_pan[0]
            id1_pan = line_split_pan[1]
            label = line_split_pan[2]
            if id0_pan in mapping_dict.keys() and id1_pan in mapping_dict.keys() and \
                    mapping_dict[id0_pan] in id_dict.keys() and mapping_dict[id1_pan] in id_dict.keys():
                uniprot_id0 = mapping_dict[id0_pan]
                id0 = id_dict[uniprot_id0]
                uniprot_id1 = mapping_dict[id1_pan]
                id1 = id_dict[uniprot_id1]
                if encoding == 'node2vec' and id0 in emd.keys() and id1 in emd.keys():
                    ppis.append([uniprot_id0, uniprot_id1, label])
                elif encoding != 'node2vec':
                    ppis.append([uniprot_id0, uniprot_id1, label])
    ppis = balance_set(ppis, id_dict, encoding, emd)
    X, y = make_X_y(np.array(ppis), emd, id_dict)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    return X_train, y_train, X_test, y_test


def read_in_richoux(path, ppis, id_dict, emd, encoding):
    from tqdm import tqdm
    lines = open(path, 'r').readlines()
    for line in tqdm(lines):
        line_split = line.strip().split(' ')
        if len(line_split) == 1:
            continue
        else:
            id0_richoux = line_split[0]
            id1_richoux = line_split[1]
            label = line_split[4]
            if id0_richoux in id_dict.keys() and id1_richoux in id_dict.keys():
                id0 = id_dict[line_split[0]]
                id1 = id_dict[line_split[1]]
                if encoding == 'node2vec' and id0 in emd.keys() and id1 in emd.keys():
                    ppis.append([id0_richoux, id1_richoux, label])
                elif encoding != 'node2vec':
                    ppis.append([id0_richoux, id1_richoux, label])
    return ppis


def load_richoux(encoding, dataset='regular'):
    datasets = {'regular', 'strict'}
    if dataset not in datasets:
        raise ValueError("dataset must be one of %r." % datasets)
    print(f'Loading {encoding} encoding ...')
    emd, id_dict = load_encoding(encoding=encoding, organism='human')
    if dataset == 'regular':
        #85,104
        path_to_train = '../DeepPPI/data/mirror/medium_1166_train_mirror.txt'
        #12,822
        path_to_val = '../DeepPPI/data/mirror/medium_1166_val_mirror.txt'
        #12,806
        path_to_test = '../DeepPPI/data/mirror/medium_1166_test_mirror.txt'
    else:
        #91,036
        path_to_train = '../DeepPPI/data/mirror/double/double-medium_1166_train_mirror.txt'
        #12,506
        path_to_val = '../DeepPPI/data/mirror/double/double-medium_1166_val_mirror.txt'
        #720
        path_to_test = '../DeepPPI/data/mirror/double/test_double_mirror.txt'
    X_train = []
    ppis_val = []
    X_test = []
    for path, ppi_list in {path_to_train: X_train, path_to_val: ppis_val, path_to_test: X_test}.items():
        ppi_list = read_in_richoux(path, ppi_list, id_dict, emd, encoding)
    # join train and val
    X_train.extend(ppis_val)
    X_train = balance_set(X_train, id_dict, encoding, emd)
    X_train, y_train = make_X_y(np.array(X_train), emd, id_dict)
    X_test = balance_set(X_test, id_dict, encoding, emd)
    X_test, y_test = make_X_y(np.array(X_test), emd, id_dict)
    return X_train, y_train, X_test, y_test


def read_in_partitions(lines, id_dict, emd, encoding, label):
    from tqdm import tqdm
    ppis = []
    for line in tqdm(lines):
        line_split = line.strip().split(' ')
        id0_uniprot = line_split[0]
        id1_uniprot = line_split[1]
        if id0_uniprot in id_dict.keys() and id1_uniprot in id_dict.keys():
            id0 = id_dict[line_split[0]]
            id1 = id_dict[line_split[1]]
            if encoding == 'node2vec' and id0 in emd.keys() and id1 in emd.keys():
                ppis.append([id0_uniprot, id1_uniprot, label])
            elif encoding != 'node2vec':
                ppis.append([id0_uniprot, id1_uniprot, label])
    return ppis


def load_partition_datasets(encoding, dataset, partition_train, partiton_test):
    dataset_choices = {'du', 'guo', 'huang', 'pan', 'richoux'}
    if dataset not in dataset_choices:
        raise ValueError('dataset must be one of %r.' % dataset_choices)
    partition_choices = {'0', '1', 'both'}
    if partition_train not in partition_choices or partiton_test not in partition_choices:
        raise ValueError('partition must be one of %r', partition_choices)
    if dataset in {'guo', 'du'}:
        organism='yeast'
    else:
        organism='human'
    print(f'Loading {encoding} encoding ...')
    emd, id_dict = load_encoding(encoding=encoding, organism=organism)
    train_pos = open(f'../SPRINT/data/{dataset}_partition_{partition_train}_pos.txt').readlines()
    X_train = read_in_partitions(lines=train_pos,
                               id_dict=id_dict, emd=emd, encoding=encoding,
                               label='1')
    train_neg = open(f'../SPRINT/data/{dataset}_partition_{partition_train}_neg.txt').readlines()
    X_train.extend(read_in_partitions(lines=train_neg,
                               id_dict=id_dict, emd=emd, encoding=encoding,
                               label='0'))
    test_pos = open(f'../SPRINT/data/{dataset}_partition_{partiton_test}_pos.txt').readlines()
    X_test = read_in_partitions(lines=test_pos,
                               id_dict=id_dict, emd=emd, encoding=encoding,
                               label='1')
    test_neg = open(f'../SPRINT/data/{dataset}_partition_{partiton_test}_neg.txt').readlines()
    X_test.extend(read_in_partitions(lines=test_neg,
                               id_dict=id_dict, emd=emd, encoding=encoding,
                               label='0'))
    X_train = balance_set(X_train, id_dict, encoding, emd)
    X_train, y_train = make_X_y(np.array(X_train), emd, id_dict)
    X_test = balance_set(X_test, id_dict, encoding, emd)
    X_test, y_test = make_X_y(np.array(X_test), emd, id_dict)
    return X_train, y_train, X_test, y_test