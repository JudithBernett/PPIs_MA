Model:
InteractionModel(
  (conv1): Conv1d(2, 20, kernel_size=(80,), stride=(20,))
  (conv2): Conv1d(20, 10, kernel_size=(80,), stride=(20,))
  (fc0): Linear(in_features=130, out_features=60, bias=True)
  (fc1): Linear(in_features=60, out_features=2, bias=True)
  (batch_norm1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True)
  (batch_norm2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True)
  (batch_norm3): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True)
)Optimizer: Adam
Epochs: 200
12:21:24
Size of batches: 9 
Learning rate: 0.003
Momentum: 0.0 
Training Loss over epochs
0 : 0.6127402257195106
1 : 0.5735406916481828
2 : 0.5485822648290823
3 : 0.5375411209156924
4 : 0.5333434965781644
5 : 0.5264902968606079
6 : 0.5213846662570899
7 : 0.5181790246926604
8 : 0.5148437168543615
9 : 0.5120940218199073
10 : 0.5107004043741528
11 : 0.5085257989405644
12 : 0.5069924052369253
13 : 0.5035543516155604
14 : 0.5014117553874305
15 : 0.5008565019258041
16 : 0.500590650912449
17 : 0.5003396195871955
18 : 0.49917801028221187
19 : 0.4983425948144497
20 : 0.50095775774976
21 : 0.49973777676927916
22 : 0.4941747154663926
23 : 0.493496205266424
24 : 0.49169203154754987
25 : 0.4910960444443941
26 : 0.4899753679815468
27 : 0.49139168634789926
28 : 0.49056325557006597
29 : 0.49024476702663905
30 : 0.4929904753847031
31 : 0.49064432253345364
32 : 0.4878991778632077
33 : 0.48652269179231444
34 : 0.4862563950051043
35 : 0.4881946487198309
36 : 0.48651849536625696
37 : 0.4869921386516679
38 : 0.48917537253469
39 : 0.4855494473790667
40 : 0.4870362379930111
41 : 0.48575329614246915
42 : 0.4866253205642207
43 : 0.4854209738506626
44 : 0.4867705316931392
45 : 0.48520951041725463
46 : 0.4844077636448223
47 : 0.4839425123283492
48 : 0.4860232261242268
49 : 0.4875157988891815
50 : 0.48580162177597724
51 : 0.48508501961547607
52 : 0.48304758073960163
53 : 0.48343875544966936
54 : 0.48259650967625123
55 : 0.48481057680944567
56 : 0.4842018465197789
57 : 0.48526972344351865
58 : 0.4835028887177754
59 : 0.487252379100614
60 : 0.4852270614680861
61 : 0.48503245203303474
62 : 0.48504558010814924
63 : 0.4850328334821538
64 : 0.48413015604706083
65 : 0.48305529015979604
66 : 0.4829302093809594
67 : 0.48436263879251434
68 : 0.48386675598278306
69 : 0.4854401735920713
70 : 0.48765116736199615
71 : 0.4872572775985365
72 : 0.4867323196352333
73 : 0.4848379912317918
74 : 0.48412761035270113
75 : 0.48391538396708306
76 : 0.4833222766249518
77 : 0.48104768765769507
78 : 0.482378456951326
79 : 0.48628425797840696
80 : 0.484488834999563
81 : 0.484246639150281
82 : 0.4841276958687823
83 : 0.48341480218386784
84 : 0.4844497660733698
85 : 0.4852266520762232
86 : 0.48422770763958684
87 : 0.4835309582564012
88 : 0.4835440835691338
89 : 0.48308511053825
90 : 0.48542823830658094
91 : 0.48451087581883695
92 : 0.48451319236797763
93 : 0.4836244859122743
94 : 0.48246041338205103
95 : 0.4836679362459798
96 : 0.48295897277299615
97 : 0.4832363494346516
98 : 0.48190661545034535
99 : 0.48193319268944157
100 : 0.48161433460016095
101 : 0.48360864335783305
102 : 0.4832765211301844
103 : 0.4821469393418657
104 : 0.4815397651710909
105 : 0.48150386222891794
106 : 0.482677686035849
107 : 0.4828539136447599
108 : 0.48278037587454886
109 : 0.4818854192918879
110 : 0.48028058168877885
111 : 0.48076198524085556
112 : 0.4816118675036614
113 : 0.48177736595758763
114 : 0.48188234296506827
115 : 0.48112793687754335
116 : 0.48036856609796824
117 : 0.48217032581891595
118 : 0.4812286746248745
119 : 0.48124212257203436
120 : 0.4819300611395172
121 : 0.481544180384662
122 : 0.48224062980654053
123 : 0.4810398373118662
124 : 0.48118250776090865
125 : 0.4817535743181117
126 : 0.4807049303579613
127 : 0.4809180855073935
128 : 0.4806787982401504
129 : 0.4815016771210704
130 : 0.4823826326227204
131 : 0.48132089054188054
132 : 0.4820840827308676
133 : 0.4802195498591628
134 : 0.48076148876772534
135 : 0.48018778640265364
136 : 0.48035503572094074
137 : 0.48165976396213833
138 : 0.47862241066106653
139 : 0.4813236844433661
140 : 0.4798935309597093
141 : 0.4788054072831389
142 : 0.48023090149703135
143 : 0.48018166605857865
144 : 0.4799993065801309
145 : 0.48099396370687414
146 : 0.48221293223759903
147 : 0.48139196623988867
148 : 0.48060851561559986
149 : 0.4800494664008577
150 : 0.4802001620206465
151 : 0.4813380016723588
152 : 0.48111294839956825
153 : 0.48039201801844555
154 : 0.48000386314129884
155 : 0.47843083482178017
156 : 0.4790461744314595
157 : 0.47978698879549053
158 : 0.4801421925489314
159 : 0.48032432208239423
160 : 0.47942009079032405
161 : 0.47805808223526375
162 : 0.47762642687466883
163 : 0.4779424008262915
164 : 0.47829217234351035
165 : 0.4778939451253214
166 : 0.4780644990815353
167 : 0.47862895760423935
168 : 0.47704836883688584
169 : 0.47752821147147473
170 : 0.47719220401505164
171 : 0.47674515809536294
172 : 0.4762109970581308
173 : 0.4776951190394558
174 : 0.47716238066814115
175 : 0.4760923581734511
176 : 0.4759990534805569
177 : 0.47760830040938923
178 : 0.4764321967640479
179 : 0.4773723176083441
180 : 0.4780515546483902
181 : 0.47914269353592126
182 : 0.4794973173903367
183 : 0.4789504598719453
184 : 0.4786102698651591
185 : 0.47964829565625383
186 : 0.47860818929114074
187 : 0.47851039505059834
188 : 0.48009877657081723
189 : 0.482204260595896
190 : 0.48140746516972577
191 : 0.4795808139004753
192 : 0.4805684620989833
193 : 0.4822860942748098
194 : 0.48101476233748175
195 : 0.4795188016608078
196 : 0.47976108337453527
197 : 0.4790354237612981
198 : 0.4782256648204497
199 : 0.478791368995837
