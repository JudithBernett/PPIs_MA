Model:
InteractionModel(
  (conv1): Conv1d(2, 20, kernel_size=(80,), stride=(20,))
  (conv2): Conv1d(20, 10, kernel_size=(80,), stride=(20,))
  (fc0): Linear(in_features=130, out_features=60, bias=True)
  (fc1): Linear(in_features=60, out_features=2, bias=True)
)
Epochs: 300
17:48:09
Size of batches: 18
Learning rate: 0.01
Momentum: 0.1
Training Loss over epochs
0 0.6925848320710412
1 0.6912802493136534
2 0.681629276566102
3 0.6407412314811426
4 0.6095695166572914
5 0.5800485770228194
6 0.5552924498743943
7 0.5344666919701971
8 0.5182475414192123
9 0.5062556036766085
10 0.4973033691830332
11 0.4906358664016956
12 0.48600227186308514
13 0.48252115884147506
14 0.4795664321308588
15 0.4773742171408591
16 0.47519091100413274
17 0.4738835566307629
18 0.4731810530098716
19 0.4724189221839352
20 0.4721140748366424
21 0.47126613751147034
22 0.47096582480783244
23 0.47031912541248155
24 0.470253921601189
25 0.4697192969548298
26 0.4695223369486912
27 0.46969795958125204
28 0.46911194427923264
29 0.4688073898425493
30 0.46884088413001757
31 0.4690892191497676
32 0.4685594302614072
33 0.46810278968666585
34 0.4679586071009537
35 0.4684599863097273
36 0.46787531814725924
37 0.46795776326443594
38 0.4678606315981137
39 0.467535161956345
40 0.4675331277147814
41 0.4677564613042711
42 0.4669500878660311
43 0.4666032837847954
44 0.46691310383161116
45 0.46664248646816847
46 0.4670743492821702
47 0.46659137696704855
48 0.46678410785976193
49 0.46671326575720434
50 0.4667970665611246
51 0.4671236537752632
52 0.46666753815120876
53 0.46666342721669973
54 0.46634979387588865
55 0.46653235478741895
56 0.4663702321162458
57 0.46599618625907785
58 0.46611119160575226
59 0.46658942533198805
60 0.46636844577269604
61 0.46618550034545936
62 0.46640936790385606
63 0.4661970060535823
64 0.46560024216177437
65 0.46545276585518625
66 0.4657478245673778
67 0.4660039682764343
68 0.46529101351981966
69 0.465685829024665
70 0.46596569682486483
71 0.46594091797487336
72 0.46581074106759995
73 0.4659036969153317
74 0.4656518156351052
75 0.4654196074301764
76 0.46560004941326716
77 0.46575947991514977
78 0.46575228281620895
79 0.46558612121890397
80 0.46558135314171434
81 0.4654210543491199
82 0.46480831883634277
83 0.4649171177144501
84 0.464687015003855
85 0.4650627048462286
86 0.46478773780586613
87 0.46486015485861526
88 0.46533193865378203
89 0.46441346952543866
90 0.46435422935806137
91 0.4646164135081932
92 0.4645696408582982
93 0.464532361587569
94 0.4643866746960245
95 0.46416098675250533
96 0.4641675615157585
97 0.4643395575481612
98 0.4640868418617864
99 0.46424032395946613
100 0.46452574258695356
101 0.4647011543069971
102 0.4642975071189354
103 0.4643105695445482
104 0.4640620571533865
105 0.4643262239577231
106 0.46463542246371065
107 0.4640710534510112
108 0.4643310477143257
109 0.4646865422881902
110 0.4642895945974349
111 0.4645001520677189
112 0.46448152339847065
113 0.4648014855376976
114 0.46455915210904125
115 0.464659851526484
116 0.464787253324319
117 0.4641641864935191
118 0.4640425399196042
119 0.46420376406596764
120 0.4635182307235183
121 0.4636772793984782
122 0.46386793446179714
123 0.4639201937810661
124 0.4637770765228887
125 0.46358425963815986
126 0.4634586983913256
127 0.463489296024553
128 0.46402803559934463
129 0.4642353236164982
130 0.4639284837430427
131 0.46381099359707334
132 0.46349331296938673
133 0.46329868550280184
134 0.46352996823620474
135 0.4633810980013024
136 0.4633251271692985
137 0.46324277111563117
138 0.46321267605926947
139 0.46341162580465617
140 0.4630568055659876
141 0.4630372312691481
142 0.4635086867186754
143 0.4630060464997491
144 0.4630967454667699
145 0.4636027431523364
146 0.46399351943665756
147 0.46366238649008423
148 0.46404655639341014
149 0.46332878159935004
150 0.46371297602556155
151 0.4637556301161848
152 0.4637807245449993
153 0.46330581495037276
154 0.463288790248592
155 0.4630110776655997
156 0.46326837847411023
157 0.46338303635926636
158 0.4634215971396713
159 0.4632068564321086
160 0.46382046577017444
161 0.46359041708058185
162 0.46312565304238346
163 0.46327188105656986
164 0.4632154930214918
165 0.46314739157465046
166 0.4631096508144194
167 0.4634268071096431
168 0.4632729493476546
169 0.4631516597637582
170 0.46288253648091904
171 0.46295297505002214
172 0.46288085977890636
173 0.4628596401917216
174 0.46290791965959727
175 0.4629197371783208
176 0.46272018537106857
177 0.46301918071785136
178 0.46278463828551913
179 0.46289109023809827
180 0.46266848761508605
181 0.4631351987547022
182 0.46279950387429986
183 0.462961479005823
184 0.46268130202492797
185 0.462689817236331
186 0.46260530053882326
187 0.46278580783777107
188 0.46298564060330194
189 0.4631796262356102
190 0.4629760272592394
191 0.46278951254022876
192 0.4627715770865413
193 0.46281647742049725
194 0.4626828666419468
195 0.46278158694770644
196 0.46296752294035465
197 0.46267743759497554
198 0.46312298391420353
199 0.4629066095153556
200 0.4628267430595791
201 0.4627250755033408
202 0.4629095501812579
203 0.4629400661009725
204 0.46273587401226474
205 0.4625427031991982
206 0.46281951069243066
207 0.4628624342698426
208 0.46285819489193625
209 0.4627943459234624
210 0.4628515252754568
211 0.4627547922851147
212 0.4627241652617142
213 0.4623504284478584
214 0.46284389852064395
215 0.46301135598415993
216 0.46265670222438476
217 0.4624420784298511
218 0.46305498940048934
219 0.46296942095674887
220 0.4626389358598698
221 0.4624466300991989
222 0.4623753997715908
223 0.46247837666739827
224 0.4624535110697842
225 0.46241126375093167
226 0.4623824473290134
227 0.4622314856815464
228 0.46229131129724554
229 0.4627178417231089
230 0.46292383280442584
231 0.4628383373994359
232 0.46269527986366027
233 0.4622872839211083
234 0.46256335845580804
235 0.46237752067245935
236 0.4625947374162369
237 0.46254639763717476
238 0.4623465843996721
239 0.4620440819466
240 0.4620404517548548
241 0.46229504235763474
242 0.46238174516392416
243 0.4621911549748737
244 0.4627926385261371
245 0.462531232033115
246 0.4623284605839239
247 0.46215961029115854
248 0.4623935139803342
249 0.46239174059986243
250 0.4623784382837086
251 0.462233978959342
252 0.46196762043314227
253 0.46226464504861187
254 0.46239517904403454
255 0.4622665791280732
256 0.46190096663141234
257 0.4621990658281507
258 0.4622328415252992
259 0.46208340625328026
260 0.46198860746996945
261 0.46214103469104406
262 0.4622727161985422
263 0.4618440600437752
264 0.46201722523590666
265 0.46191288333093017
266 0.46178636192333394
267 0.4622594072094943
268 0.46220959147458646
269 0.4620421449330279
270 0.46188998164453904
271 0.46163183005185043
272 0.46200466373856225
273 0.4621050770360465
274 0.461969825932569
275 0.46191793590892016
276 0.4622535395963749
277 0.4618364178039229
278 0.46173514955170186
279 0.46192634472691546
280 0.46149343109892355
281 0.46159506918648413
282 0.4618451376867247
283 0.4620200307364522
284 0.46188224520893684
285 0.4619478011037081
286 0.4619524537496438
287 0.4618262433381625
288 0.46172936829368183
289 0.461513212805011
290 0.46168175496708064
291 0.4616976836754532
292 0.46166547089766075
293 0.4616757027699361
294 0.4617197517211162
295 0.4614527816438031
296 0.4612791159159265
297 0.4614798601497189
298 0.46139697416032033
299 0.46143429759972027
Testing Loss over epochs
