# Get PRoBERTa running

## Get started
```
conda create -n proberta python=3.8
conda activate proberta
pip install numpy==1.20.3
pip install sentencepiece
pip install pandas==2.0.2
wget https://github.com/imonlius/fairseq/archive/refs/heads/master.zip
unzip master.zip
mv fairseq-master fairseq
cd fairseq
pip install --editable . --no-binary cffi
wget https://github.com/NVIDIA/apex/archive/refs/heads/master.zip
unzip master.zip
mv apex-master/ apex/
cd apex
pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option="--cpp_ext" --global-option="--cuda_ext" ./
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```
1. Create JSON files from input files with ```create_data_proberta.py```
2. Tokenize data with tokenizer.py
3. Shuffle and split
4. Execute script from README
```bash
# Split data into from sequence, to sequence, and label files
for f in data/original/guo_train_Finetune*; do
        cut -f1 -d',' "$f" > tmpout/from/$(basename "$f").from
        cut -f2 -d',' "$f" > tmpout/to/$(basename "$f").to
	cut -f3 -d',' "$f" > tmpout/label/$(basename "$f").label
done

# Binarize sequences
fairseq-preprocess \
        --only-source \
        --trainpref tmpout/from/guo_train_Finetune_int_data.split.train.80.from \
        --validpref tmpout/from/guo_train_Finetune_int_data.split.valid.10.from \
        --testpref tmpout/from/guo_train_Finetune_int_data.split.test.10.from \
        --destdir tmpout/split_binarized/input0 \
        --workers 60 \
        --srcdict Pretraining/split_binarized/dict.txt

fairseq-preprocess \
        --only-source \
        --trainpref tmpout/to/guo_train_Finetune_int_data.split.train.80.to \
        --validpref tmpout/to/guo_train_Finetune_int_data.split.valid.10.to \
        --testpref tmpout/to/guo_train_Finetune_int_data.split.test.10.to \
        --destdir tmpout/split_binarized/input1 \
        --workers 60 \
        --srcdict Pretraining/split_binarized/dict.txt

# Binarize labels
fairseq-preprocess \
	--only-source \
	--trainpref tmpout/label/guo_train_Finetune_int_data.split.train.80.label \
        --validpref tmpout/label/guo_train_Finetune_int_data.split.valid.10.label \
        --testpref tmpout/label/guo_train_Finetune_int_data.split.test.10.label \
	--destdir tmpout/split_binarized/label \
	--workers 60
```
5. Execute fine-tuning
```bash
bash pRoBERTa_finetune_ppi.sh ppi 3 ppi_prediction \
        tmopout/split_binarized/ \
        768 5 12500 312 0.0025 32 64 2 3 \
        Pretraining/checkpoint_best.pt \
        no 1
```
Arguments

| Name | Description | Example                                                                                                                                   |
| ----- | ------------------------------------------ |-------------------------------------------------------------------------------------------------------------------------------------------|
| PREFIX | Prefix for the model output files | ppi                                                                                                                                       |
| NUM_GPUS | Number of GPUs to use for finetuning | 3                                                                                                                                         |
| OUTPUT_DIR | Model output directory | [ppi_prediction](https://drive.google.com/drive/u/2/folders/1mS34_2YTBh2wZuvn9QF7m0254bnc2LE_)                                            |
| DATA_DIR | Binarized input data directory | [ppi_prediction/split_binarized/robustness_minisplits/1.00](https://drive.google.com/drive/u/2/folders/1kjNnud51AIPu_eeuqdapHHE-GVoaHfZm) |
| ENCODER_EMBED_DIM | Dimension of embedding generated by the encoders | 768                                                                                                                                       |
| ENCODER_LAYERS | Number of encoder layers in the model | 5                                                                                                                                         |
| TOTAL_UPDATES | Total (maximum) number of updates during training | 12500                                                                                                                                     |
| WARMUP_UPDATES | Total number of LR warm-up updates during training | 3125                                                                                                                                      |
| PEAK_LEARNING_RATE | Peak learning rate for training | 0.0025                                                                                                                                    |
| MAX_SENTENCES | Maximum number of sequences in each batch | 32                                                                                                                                        |
| UPDATE_FREQ | Updates the model every UPDATE_FREQ batches | 64                                                                                                                                        |
| PATIENCE | Early stop training if valid performance doesnâ€™t improve for PATIENCE consecutive validation runs | 3                                                                                                                                         |
| PRETRAIN_CHECKPOINT | Path to pretrained model checkpoint | [pretraining/checkpoint_best.pt](https://drive.google.com/drive/u/2/folders/1TbFjyRfbkLgJ_rlvO1SFB-ZvwQyykvK7)                            |
| RESUME_TRAINING | Whether to resume training from previous finetuned model checkpoints | no                                                                                                                                        |
