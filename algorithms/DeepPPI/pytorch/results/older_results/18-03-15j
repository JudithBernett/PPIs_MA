Model:
InteractionModel(
  (conv1): Conv1d(2, 20, kernel_size=(80,), stride=(20,))
  (conv2): Conv1d(20, 10, kernel_size=(80,), stride=(20,))
  (fc0): Linear(in_features=130, out_features=60, bias=True)
  (fc1): Linear(in_features=60, out_features=2, bias=True)
)
Epochs: 100
07:21:49
Size of batches: 9 
Learning rate: 0.0003
Momentum: 0.0 
Training Loss over epochs
0 : 0.6936522141211826
 1 : 0.6932795764311074
 2 : 0.6930500088081102
 3 : 0.6929188181508479
 4 : 0.6928380301978425
 5 : 0.6927808747505098
 6 : 0.692733285181374
 7 : 0.6926925897029108
 8 : 0.6926573638583156
 9 : 0.6926263055893243
 10 : 0.6925976716906843
 11 : 0.6925694135348578
 12 : 0.6925418954226074
 13 : 0.6925140841642335
 14 : 0.6924841023409959
 15 : 0.6924555638451148
 16 : 0.6924245320616249
 17 : 0.6923911757460541
 18 : 0.6923565744352764
 19 : 0.6923193237500478
 20 : 0.6922793178955583
 21 : 0.6922356124512256
 22 : 0.6921890697119697
 23 : 0.6921399175571226
 24 : 0.6920860207194672
 25 : 0.692028154569666
 26 : 0.6919656459108402
 27 : 0.6918975387331605
 28 : 0.6918242524672701
 29 : 0.6917441586915074
 30 : 0.6916551139447341
 31 : 0.6915573186394727
 32 : 0.691451596289353
 33 : 0.6913357335346719
 34 : 0.691209122033193
 35 : 0.6910772274837393
 36 : 0.6909297742869535
 37 : 0.6907656007999254
 38 : 0.6905816164227117
 39 : 0.6903882099289151
 40 : 0.6901723831195284
 41 : 0.6899377846446993
 42 : 0.6896851105363423
 43 : 0.6894186066784355
 44 : 0.6891383574929016
 45 : 0.6888406274642134
 46 : 0.6885218193235074
 47 : 0.6881804573995172
 48 : 0.6878107676762752
 49 : 0.6874091947278892
 50 : 0.6869574756774613
 51 : 0.6864372154070352
 52 : 0.6859023834299327
 53 : 0.6852617452990117
 54 : 0.6845309038695278
 55 : 0.6836802221007907
 56 : 0.6826964499705475
 57 : 0.681524166797469
 58 : 0.680081854947586
 59 : 0.6783290104152421
 60 : 0.6761434372118283
 61 : 0.673404963659463
 62 : 0.6700285033230566
 63 : 0.6659575983670969
 64 : 0.6613367681229315
 65 : 0.6562525973356904
 66 : 0.6512263949112865
 67 : 0.6466350682891824
 68 : 0.6424729134059478
 69 : 0.638816594817468
 70 : 0.6354351397658948
 71 : 0.6324993187722934
 72 : 0.6296471901726165
 73 : 0.6269507088088503
 74 : 0.6243859017448752
 75 : 0.6217041515797661
 76 : 0.6192033326729842
 77 : 0.6167955005889195
 78 : 0.6142987897806667
 79 : 0.6119605145507004
 80 : 0.6096827531031649
 81 : 0.6073057703823489
 82 : 0.605144407091111
 83 : 0.6028568132519526
 84 : 0.6004072637111685
 85 : 0.5982467523865845
 86 : 0.5957565615944145
 87 : 0.5937186338947052
 88 : 0.5914680772326987
 89 : 0.5892838711139203
 90 : 0.5869320676854303
 91 : 0.5847245318778926
 92 : 0.5824008593933733
 93 : 0.5801341728352386
 94 : 0.5778144671359972
 95 : 0.5756408659042714
 96 : 0.5735051899235776
 97 : 0.5711522133708746
 98 : 0.5689175326523123
 99 : 0.5667047746820516
 Testing Loss over epochs
0 / 5.0935714735886937e-05 
1 / 5.070496794478055e-05 
2 / 5.05359773790065e-05 
3 / 5.042275194753783e-05 
4 / 5.0346523898244044e-05 
5 / 5.0285606306491216e-05 
6 / 5.023699164285375e-05 
7 / 5.020343514904306e-05 
8 / 5.017500966113415e-05 
9 / 5.015384793545714e-05 
10 / 5.0134269472589436e-05 
11 / 5.0114800337333416e-05 
12 / 5.010312648012726e-05 
13 / 5.009008843620394e-05 
14 / 5.00871817538358e-05 
15 / 5.006961909235685e-05 
16 / 5.005672028772394e-05 
17 / 5.004097042113573e-05 
18 / 5.003098796130397e-05 
19 / 5.0028478616737784e-05 
20 / 5.0026226863024106e-05 
21 / 5.002719681979538e-05 
22 / 5.0018035197397746e-05 
23 / 5.001466648436392e-05 
24 / 5.000606690465251e-05 
25 / 5.000338206896322e-05 
26 / 4.99963115317789e-05 
27 / 5.000205273075741e-05 
28 / 4.99959603603173e-05 
29 / 4.998750817817804e-05 
30 / 4.997452536474977e-05 
31 / 4.99528446839072e-05 
32 / 4.994209118030276e-05 
33 / 4.991621219211321e-05 
34 / 4.991742283127708e-05 
35 / 4.9888735881416154e-05 
36 / 4.9860062689402254e-05 
37 / 4.982341305207048e-05 
38 / 4.9765247287535625e-05 
39 / 4.9757491014674935e-05 
40 / 4.971031708163309e-05 
41 / 4.965071250450312e-05 
42 / 4.9572007795142304e-05 
43 / 4.954079642346536e-05 
44 / 4.950015527629475e-05 
45 / 4.9460136320631145e-05 
46 / 4.9399700894877746e-05 
47 / 4.934549599461518e-05 
48 / 4.924292540923261e-05 
49 / 4.9236879161737816e-05 
50 / 4.9287241100661785e-05 
51 / 4.897824794163711e-05 
52 / 4.909293206672661e-05 
53 / 4.900558908995163e-05 
54 / 4.89551470216283e-05 
55 / 4.8991216921596726e-05 
56 / 4.895778726022154e-05 
57 / 4.9091559091861e-05 
58 / 4.9169522872383116e-05 
59 / 4.896661596007988e-05 
60 / 4.8821877481749336e-05 
61 / 4.875243389531821e-05 
62 / 4.90610959521648e-05 
63 / 4.887268667179118e-05 
64 / 4.871860291463791e-05 
65 / 4.925246448298203e-05 
66 / 4.969504330381493e-05 
67 / 4.924585534755264e-05 
68 / 4.956926578606902e-05 
69 / 4.846613936666714e-05 
70 / 5.0353186765763246e-05 
71 / 4.8801835167211676e-05 
72 / 5.0337771541244944e-05 
73 / 5.0320404211072165e-05 
74 / 4.9871948102263376e-05 
75 / 4.9971994163084795e-05 
76 / 4.91448190893245e-05 
77 / 4.989465728850306e-05 
78 / 4.991221175819682e-05 
79 / 4.937283561420487e-05 
80 / 5.049859411470274e-05 
81 / 5.027317262027903e-05 
82 / 4.902412764439201e-05 
83 / 4.9506304493694185e-05 
84 / 5.008964444769624e-05 
85 / 4.969589285764287e-05 
86 / 5.045102601170278e-05 
87 / 4.977484899000841e-05 
88 / 4.955098390160293e-05 
89 / 4.9267144730806476e-05 
90 / 4.946718518737547e-05 
91 / 4.9574052040430975e-05 
92 / 5.06983960816097e-05 
93 / 4.914037691387394e-05 
94 / 4.940202105396693e-05 
95 / 5.036963324999064e-05 
96 / 4.979614912217337e-05 
97 / 5.0449535924268096e-05 
98 / 5.027626243131107e-05 
99 / 5.0921366007916875e-05 
