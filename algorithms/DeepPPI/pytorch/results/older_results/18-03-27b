Model:
InteractionModel(
  (conv1): Conv1d(2, 20, kernel_size=(80,), stride=(20,))
  (conv2): Conv1d(20, 10, kernel_size=(80,), stride=(20,))
  (fc0): Linear(in_features=130, out_features=60, bias=True)
  (fc1): Linear(in_features=60, out_features=2, bias=True)
  (batch_norm1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True)
  (batch_norm2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True)
  (batch_norm3): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True)
)Optimizer: Adam
Epochs: 100
06:01:16
Size of batches: 9 
Learning rate: 0.003
Momentum: 0.0 
Training Loss over epochs
0 : 0.6051445113648934
1 : 0.6073082973680726
2 : 0.6074923623057624
3 : 0.6106752235029024
4 : 0.6038897726237087
5 : 0.5989015815552026
6 : 0.6017051178460299
7 : 0.6105278325063208
8 : 0.6173296696777834
9 : 0.6024074388258859
10 : 0.604737863744369
11 : 0.6008572477993307
12 : 0.5985315090531693
13 : 0.6056352692595194
14 : 0.6055166719185117
15 : 0.6018020176341905
16 : 0.6046455696171195
17 : 0.6038254269662169
18 : 0.6208523618211312
19 : 0.610059407875457
20 : 0.6141935340130372
21 : 0.596632246613738
22 : 0.6073233201133211
23 : 0.6143921347660652
24 : 0.607428426290367
25 : 0.6185673326597348
26 : 0.6075828788868879
27 : 0.6138309077787525
28 : 0.6061242670017047
29 : 0.6155946756963633
30 : 0.5984991431079257
31 : 0.606604677617334
32 : 0.6163255337884562
33 : 0.6115380700997785
34 : 0.608531316213008
35 : 0.6046537318962124
36 : 0.6031962680137585
37 : 0.6032503778050516
38 : 0.5981884499619892
39 : 0.6069994318002456
40 : 0.5988812770523992
41 : 0.6031020995932345
42 : 0.6036395365994818
43 : 0.6204634465431751
44 : 0.6012794404613528
45 : 0.6009549522109435
46 : 0.6150643653360527
47 : 0.6015403619232953
48 : 0.5958403477768464
49 : 0.6017042399139675
50 : 0.6024640202031624
51 : 0.5957021370191578
52 : 0.6016762810963174
53 : 0.601975152027972
54 : 0.5978743700279309
55 : 0.600681264727454
56 : 0.5970274344120993
57 : 0.598131288975173
58 : 0.6001660565443638
59 : 0.5960926058646052
60 : 0.6029612036580744
61 : 0.6020960488139229
62 : 0.5960654433538225
63 : 0.5980813558414888
64 : 0.6076695213132993
65 : 0.6150191855893781
66 : 0.5998556192597552
67 : 0.6011533694348525
68 : 0.6090494098781479
69 : 0.6010810802563421
70 : 0.5961296286754086
71 : 0.596556603074427
72 : 0.6037680165407333
73 : 0.599155772569386
74 : 0.5982680160737485
75 : 0.6054363408134217
76 : 0.6073817999387674
77 : 0.6002829273034759
78 : 0.6010336010303786
79 : 0.5996710520101528
80 : 0.5980160677113108
81 : 0.6053726116233961
82 : 0.5977724577187706
83 : 0.6096563112060609
84 : 0.5974127135613971
85 : 0.602678041494242
86 : 0.6232520618994534
87 : 0.6085671511554467
88 : 0.6044875456042929
89 : 0.6049043248466728
90 : 0.5955299252020437
91 : 0.606026553463928
92 : 0.6053443459213114
93 : 0.5967126947888662
94 : 0.6012188775742725
95 : 0.6021182815623118
96 : 0.598669583889894
97 : 0.6032470208879439
98 : 0.6042781241745944
99 : 0.6067348636066124
