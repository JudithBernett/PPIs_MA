Model:
InteractionModelFC(
  (fc1): Linear(in_features=23320, out_features=1000, bias=True)
  (fc2): Linear(in_features=1000, out_features=200, bias=True)
  (fc3): Linear(in_features=400, out_features=200, bias=True)
  (fc4): Linear(in_features=200, out_features=100, bias=True)
  (fc5): Linear(in_features=100, out_features=2, bias=True)
  (norm1a): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True)
  (norm1b): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True)
  (norm3): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True)
  (norm4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True)
)
Optimizer: SGD
Epochs: 150
09:37:09
Size of batches: 18 
Learning rate: 0.001
Momentum: 0.1 
Training Loss over epochs
0 : 0.574386250425178
1 : 0.4922530587798167
2 : 0.4726480617604053
3 : 0.4647863379406387
4 : 0.4608383379879067
5 : 0.4578877979982123
6 : 0.45635447990435063
7 : 0.45474502409921497
8 : 0.4539997309573905
9 : 0.45286809906466524
10 : 0.45202043078469334
11 : 0.4517115418813634
12 : 0.45121230442625226
13 : 0.4504588702163062
14 : 0.4501148813014993
15 : 0.4497606277092923
16 : 0.44958888964312305
17 : 0.44889929562875774
18 : 0.4482930512521704
19 : 0.448120284235552
20 : 0.4489121005493518
21 : 0.44740764852177756
22 : 0.4471776135214309
23 : 0.44729600223146226
24 : 0.44618518674573815
25 : 0.4465042080265305
26 : 0.44609950189924885
27 : 0.44602520724131084
28 : 0.44530308979649585
29 : 0.44531644697757705
30 : 0.4457568072805603
31 : 0.445175571105258
32 : 0.44484194278089007
33 : 0.44492208351223805
34 : 0.44445892588496405
35 : 0.44402651821935024
36 : 0.4441528922246795
37 : 0.4448443554147985
38 : 0.4441955382783907
39 : 0.44432925771717025
40 : 0.4436696505699654
41 : 0.44385231789452956
42 : 0.44366465506484565
43 : 0.4436615154681961
44 : 0.4434406521252044
45 : 0.4436258511850227
46 : 0.44343112437082116
47 : 0.4435964364694615
48 : 0.4429875124548861
49 : 0.4429525083171956
50 : 0.4433645740283882
51 : 0.44278090469021525
52 : 0.44261632343048246
53 : 0.4427570854514533
54 : 0.44224296799214763
55 : 0.4424062403048345
56 : 0.4428014417679089
57 : 0.44202106267675034
58 : 0.442350561382506
59 : 0.44202462097787526
60 : 0.4414531554699258
61 : 0.44262980943767105
62 : 0.44201347741163755
63 : 0.44181094066186843
64 : 0.4426707848440396
65 : 0.4410473609838118
66 : 0.4415356489361451
67 : 0.44140285733384366
68 : 0.4418963028837436
69 : 0.44150101551697135
70 : 0.4409154371815682
71 : 0.44143572677347587
72 : 0.44108959481949317
73 : 0.4414504817688508
74 : 0.44098112867347183
75 : 0.4408444815613072
76 : 0.4409388712078951
77 : 0.44076898214178806
78 : 0.44058147535969405
79 : 0.4410373889913204
80 : 0.441136121759691
81 : 0.4406574248757298
82 : 0.4410263009474864
83 : 0.4403398961937659
84 : 0.44047311366409075
85 : 0.44066536951543944
86 : 0.44102383117201616
87 : 0.44055376583351524
88 : 0.44051955889077576
89 : 0.4403257020060703
90 : 0.4401421067763364
91 : 0.44015645744339743
92 : 0.4399841372849794
93 : 0.44024496988948725
94 : 0.44049584977439177
95 : 0.44032244644826046
96 : 0.44029124202326325
97 : 0.4397682204602147
98 : 0.44037158391292763
99 : 0.4404680710463764
100 : 0.44033085918167447
101 : 0.4402515025153771
102 : 0.44011094394898315
103 : 0.44039287476269556
104 : 0.4396629445659592
105 : 0.43995064852227495
106 : 0.4399046044444661
107 : 0.43978613368727715
108 : 0.43957856163053793
109 : 0.44024767262746445
110 : 0.44010484124391847
111 : 0.4396596337212596
112 : 0.4401519683029921
113 : 0.43954027148505204
114 : 0.4395424916844718
115 : 0.439742366714936
116 : 0.4390271090738939
117 : 0.439099342288412
118 : 0.43921824895240463
119 : 0.4395337760997988
120 : 0.4394060652684647
121 : 0.4392033922248347
122 : 0.43913440707093365
123 : 0.439344794205784
124 : 0.4393027047793803
125 : 0.43925361344599984
126 : 0.439207712217189
127 : 0.4391941712960076
128 : 0.4391756094440175
129 : 0.4388792819300882
130 : 0.43936562722632866
131 : 0.43888014609930665
132 : 0.4394030802376301
133 : 0.439031406692427
134 : 0.4388163285817525
135 : 0.4395779026037461
136 : 0.439299310876776
137 : 0.43907870863039006
138 : 0.4393151488043503
139 : 0.4391431864896887
140 : 0.43884091455566676
141 : 0.43896360213049235
142 : 0.4387518873056142
143 : 0.43903144946764044
144 : 0.4389989829137995
145 : 0.43900689523903835
146 : 0.43943054100224405
147 : 0.4392781958087008
148 : 0.4388018374698979
149 : 0.43924194344730016
